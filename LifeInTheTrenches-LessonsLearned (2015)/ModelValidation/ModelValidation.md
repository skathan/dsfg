##Model Validation
*Repeating what you just heard does not mean that you learned anything.*

Model validation is central to construction of any model. This answers the question “How well did my hypothesis fit the observed data?”

If we do not have enough data, our models cannot connect the dots. On the other hand, given too much data the model cannot think outside of the box. The model learns specific details about the training data that do not generalize to the population. This is the problem of model over fitting. 

Many techniques exist to combat model over fitting. The simplest method is to split your dataset into training, testing and validation sets. The training data is used to construct the model. The model constructed with the training data is then evaluated with the testing data. The performance of the model against the testing set is used to further reduce model error. This indirectly includes the testing data within model construction, helping to reduce model over fit. Finally, the model is evaluated on the validation data to assess how well the model generalizes.

A few methods where the data is split into training and testing sets include: k-fold cross-validation, Leave-One-Out cross-validation, bootstrap methods, and resampling methods. Leave-One-Out cross-validation can be used to get a sense of ideal model performance over the training set. A sample is selected from the data to act as the testing sample and the model is trained on the rest of the data. The error on the test sample is calculated and saved, and the sample is returned to the dataset. A different sample is then selected and the process is repeated. This continues until all samples in the testing set have been used. The average error over the testing examples gives a measure of the model’s error.

There are other approaches for testing how well your hypothesis reflects the data. Statistical methods such as calculating the coefficient of determination, commonly called the R-squared value are used to identify how much variation in the data your model explains. Note that as the dimensionality of your feature space grows, the R-squared value also grows. An adjusted R-squared value compensates for this phenomenon by including a penalty for model complexity. When testing the significance of the regression as a whole, the F-test compares the explained variance to unexplained variance. A regression result with a high F-statistic and an adjusted R-squared over 0.7 is almost surely significant.
