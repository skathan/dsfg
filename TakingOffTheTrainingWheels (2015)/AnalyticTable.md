 
 
| Technique       | Description          | Tips From the Pros  | References We Love to Read |
| ------------- |:-------------:| :-------------:|-----:|
|	Active Learning	|	Intelligent sample selection to improve performance of model. Samples are selected to provide the greatest information to a learning model.	|	Can be paired with a human in-the-loop to help capture domain knowledge.	|	Burr, Settles B. “Active Learning: Synthesis Lectures on Arti?cial Intelligence and Machine Learning.” Morgan & Claypool, 2012. Print.	|
|	Agent Based Simulation	|	Simulates the actions and interactions of autonomous agents.	|	In many systems, complex behavior results from surprisingly simple rules. Keep the logic of your agents simple and gradually build in sophistication.	|	Macal, Charles, and Michael North. “Agent-based Modeling and Simulation.” Winter Simulation Conference. Austin, TX. 2009. Conference Presentation.	|
|	ANOVA	|	Hypothesis testing for differences between more than two groups.	|	Check model assumptions before utilizing, and watch out for Family Wise error when running multiple tests.	|	Bhattacharyya, Gouri K., and Richard A. Johnson. Statistical Concepts and Models. Wiley, 1977. Print.	|
|	Association Rule Mining (Apriori)	|	Data mining technique to identify the common co-occurances of items.	|	Utilize when you have a need to understand potential relationships between elements.	|	Agrawal, Rakesh, and Ramakrishnan Srikant.  “Fast Algorithms for Mining Association Rules.” Proc. Of 20th Intl. Conf. on VLDB. 1994. Conference Presentation.	|
|	Bayesian Network	|	Models conditional probabilities amongst elements, visualized as a Directed Acyclic Graph.	|	Calculate by hand before using larger models to ensure understanding.	|	Russel, Stuart, and Peter Norvig. “Arti?cial Intelligence: A Modern Approach.” Prentice Hall, 2009 Print.	|
|	Collaborative Filtering	|	Also known as 'Recommendation,' suggest or eliminate items from a set by comparing a history of actions against items performed by users. Finds similar items based on who used them or similar users based on the items they use.	|	Use Singular Value Decomposition based Recommendation for cases where there are latent factors in your domain, e.g., genres in movies.	|	Owen, Sean, Robin Anil, Ted Dunning, and Ellen Friedman. Mahout in Action. New Jersey: Manning, 2012. Print.	|
|	Coordinate Transforma- tion	|	Provides a different perspective on data.	|	Changing the coordinate system for data, for example, using polar or cylindrical coordinates, may more readily highlight key structure in the data. A key step in coordinate transformations is to appreciate multidimensionality and to systematically analyze subspaces of the data.	|	Abbott, Edwin A., Flatland: A Romance of Many Dimensions. United Kingdom: Seely & Co., 1884. Print.	|
|	Deep Learning	|	Method that learns features that leads to higher concept learning. Usually very deep neural network architectures.	|	Utilize a GPU to ef?ciently train complex models.	|	Bengio, Yoshua, and Yann LeCun. “Scaling Learning Algorithms towards AI.” Large- Scale Kernel Machines. New York: MIT Press, 2007. Print.	|
|	Design of Experiments	|	Applies controlled experiments to quantify effects on system output caused by changes to inputs.	|	Fractional factorial designs can signi?cantly reduce the number of different types of experiments you must conduct.	|	Montgomery, Douglas. Design and Analysis of Experiments. New Jersey: John Wiley & Sons, 2012. Print.	|
|	Differential Equations	|	Used to express relationships between functions and their derivatives, for example, change over time.	|	Differential equations can be used to formalize models and make predictions. The equations themselves can be solved numerically and tested with different initial conditions to study system trajectories.	|	Zill, Dennis, Warren Wright, and Michael Cullen. Differential Equations with Boundary-Value Problems. Connecticut: Cengage Learning, 2012. Print.	|
|	Discrete Event Simulation	|	Simulates a discrete sequence of events where each event occurs at a particular instant in time. The model updates its state only at points in time when events occur.	|	Discrete event simulation is useful when analyzing event based processes such as production lines and service centers to determine how system level behavior changes as different process parameters change. Optimization can integrate with simulation to gain ef?ciencies in a process.	|	Burrus, C. Sidney, Ramesh A. Gopinath, Haitao Guo, Jan E. Odegard and Ivan W. Selesnick. Introduction to Wavelets and Wavelet Transforms: A Primer. New Jersey: Prentice Hall, 1998. Print.	|
|	Discrete Wavelet Transform	|	Transforms time series data into frequency domain preserving locality information.	|	Offers very good time and frequency localization. The advantage over Fourier transforms is that it preserves both frequency and locality.	|	Burrus, C.Sidney, Ramesh A. Gopinath, Haitao Guo, Jan E. Odegard, and Ivan W. Selesnick. Introduction to Wavelets and Wavelet Transforms: A Primer. New Jersey: Prentice Hall, 1998. Print.	|
|	Ensemble Learning	|	Learning multiple models and combining output to achieve better performance.	|	Be careful not to over?t data by having too many model parameters and overtraining.	|	Dietterich, Thomas G. “Ensemble Methods in Machine Learning.” Lecture Notes in Computer Science. Springer, 2000. Print.	|
|	Expert Systems	|	Systems that use symbolic logic to reason about facts. Emulates human reasoning.	|	Useful to have a human readable explanation of why a system came to a conclusion.	|	Shortliffe, Edward H., and Bruce G. Buchanan. “A Model of Inexact Reasoning in Medicine.” Mathematical Biosciences. Elsevier B.V., 1975. Print.	|
|	Exponential Smoothing	|	Used to remove artifacts expected from collection error or outliers.	|	In comparison to a using moving average where past observations are weighted equally, exponential smoothing assigns exponentially decreasing weights over time.	|	Chat?eld, Chris, Anne B. Koehler, J. Keith Ord, and Ralph D. Snyder. “A New Look at Models for Exponential Smoothing.” Journal of the Royal Statistical Society: Series D (The Statistician). Royal Statistical Society, 2001. Print.	|
|	Factor Analysis	|	Describes variability among correlated variables with the goal of lowering the number of unobserved variables, namely, the factors.	|	If you suspect there are inmeasurable in?uences on your data, then you may want to try factor analysis.	|	Child, Dennis. The Essentials of Factor Analysis. United Kingdom: Cassell Educational, 1990. Print.	|
|	Fast Fourier Transform	|	Transforms time series from time to frequency domain ef?ciently. Can also be used for image improvement by spatial transforms.	|	Filtering a time varying signal can be done more effectively in the frequency domain. Also, noise can often be identi?ed in such signals by observing power at aberrant frequencies.	|	Mitra, Partha P., and Hemant Bokil. Observed Brain Dynamics. United Kingdom: Oxford University Press, 2008. Print.	|
|	Format Conversion	|	Creates a standard representation of data regardless of source format. For example, extracting raw UTF-8 encoded text from binary ?le formats such as Microsoft Word or PDFs.	|	There are a number of open source software packages that support format conversion and can interpret a wide variety of formats. One notable package is Apache Tikia.	|	Ingersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.	|
|	Fuzzy Logic	|	Logical reasoning that allows for degrees of truth for a statement.	|	Utilize when categories are not clearly de?ned. Concepts such as "warm", "cold", and "hot" can mean different things at different temperatures and domains.	|	Zadeh L.A., "Fuzzy Sets.” Information and Control. California: University of California, Berkeley, 1965. Print.	|
|	Gaussian Filtering	|	Acts to remove noise or blur data.	|	Can be used to remove speckle noise from images.	|	Parker, James R. Algorithms for Image Processing and Computer Vision. New Jersey: John Wiley & Sons, 2010. Print.	|
|	Generalized Linear Models	|	Expands ordinary linear regression to allow for error distribution that is not normal.	|	Use if the observed error in your system does not follow the normal distribution.	|	MacCullagh, P., and John A. Nelder. Generalized Linear Models. Florida: CRC Press, 1989. Print.	|
|	Genetic Algorithms	|	Evolves candidate models over generations by evolutionary inspired operators of mutation and crossover of parameters.	|	Increasing the generation size adds diversity in considering parameter combinations, but requires more objective function evaluation. Calculating individuals within a generation is strongly parallelizable. Representation of candidate solutions can impact performance.	|	De Jong, Kenneth A. Evolutionary Computation - A Uni?ed Approach. Massachusetts: MIT Press, 2002. Print.	|
|	Grid Search	|	Systematic search across discrete parameter values for parameter exploration problems.	|	A grid across the parameters is used to visualize the parameter landscape and assess whether multiple minima are present.	|	Kolda, Tamara G., Robert M. Lewis, and Virginia Torczon. “Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods.” SIAM Review. Society for Industrial and Applied Mathematics, 2003. Print.	|
|	Hidden Markov Models	|	Models sequential data by determining the discrete latent variables, but the observables may be continuous or discrete.	|	One of the most powerful properties of Hidden Markov Models is their ability to exhibit some degree of invariance to local warping (compression and stretching) of the time axis. However, a signi?cant weakness of the Hidden Markov Model is the way in which it represents the distribution of times for which the system remains in a given state.	|	Bishop, Christopher M. Pattern Recognition and Machine Learning. New York: Springer, 2006. Print.	|
|	Hierarchical Clustering	|	Connectivity based clustering approach that sequentially builds bigger (agglomerative) or smaller (divisive) clusters in the data.	|	Provides views of clusters at multiple resolutions of closeness. Algorithms begin to slow for larger datasets due to most implementations exhibiting O(N3) or O(N2) complexity.	|	Rui Xu, and Don Wunsch. Clustering. New Jersey: Wiley- IEEE Press, 2008. Print.	|
|	K-means and X-means Clustering	|	Centroid based clustering algorithms, where with K means the number of clusters is set and X means the number of clusters is unknown.	|	When applying clustering techniques, make sure to understand the shape of your data. Clustering techniques will return poor results if your data is not circular or ellipsoidal shaped.	|	Rui Xu, and Don Wunsch. Clustering. New Jersey: Wiley- IEEE Press, 2008. Print.	|
|	Linear, Non-linear, and Integer Programming	|	Set of techniques for minimizing or maximizing a function over a constrained set of input parameters.	|	Start with linear programs because algorithms for integer and non-linear variables can take much longer to run.	|	Winston, Wayne L. Operations Research: Applications and Algorithms. Connecticut: Cengage Learning, 2003. Print.	|
|	Markov Chain Monte Carlo (MCMC)	|	A method of sampling typically used in Bayesian models to estimate the joint distribution of parameters given the data.	|	Problems that are intractable using analytic approaches can become tractable using MCMC, when even considering high-dimensional problems. The tractability is a result of using statistics on the underlying distributions of interest, namely, sampling with Monte Carlo and considering the stochastic sequential process of Markov Chains.	|	Andrieu, Christophe, Nando de Freitas, Amaud Doucet, and Michael I. Jordan. “An Introduction to MCMC for Machine Learning.” Machine Learning. Kluwer Academic Publishers, 2003. Print.	|
|	Monte Carlo Methods	|	Set of computational techniques to generate random numbers.	|	Particularly useful for numerical integration, solutions of differential equations, computing Bayesian posteriors, and high dimensional multivariate sampling.	|	Fishman, George S. Monte Carlo: Concepts, Algorithms, and Applications. New York: Springer, 2003. Print.	|
|	Naïve Bayes	|	Predicts classes following Bayes Theorem that states the probability of an outcome given a set of features is based on the probability of features given an outcome.	|	Assumes that all variables are independent, so it can have issues learning in the context of highly interdependent variables. The model can be learned on a single pass of data using simple counts and therefore is useful in determining whether exploitable patterns exist in large datasets with minimal development time.	|	Ingersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.	|
|	Neural Networks	|	Learns salient features in data by adjusting weights between nodes through a learning rule.	|	Training a neural network takes substantially longer than evaluating new data with an already trained network. Sparser network connectivity can help to segment the input space and improve performance on classi?cation tasks.	|	Haykin, Simon O. Neural Networks and Learning Machines. New Jersey: Prentice Hall, 2008. Print.	|
|	Outlier Removal	|	Method for identifying and removing noise or artifacts from data.	|	Be cautious when removing outliers. Sometimes the most interesting behavior of a system is at times when there are aberrant data points.	|	Maimon, Oded, and Lior Rockach. Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers. The Netherlands: Kluwer Academic Publishers, 2005. Print.	|
|	Principal Components Analysis	|	Enables dimensionality reduction by identifying highly correlated dimensions.	|	Many large datasets contain correlations between dimensions; therefore part of the dataset is redundant. When analyzing the resulting principal components, rank order them by variance as this is the highest information view of your data. Use skree plots to infer the optimal number of components.	|	Wallisch, Pascal, Michael E. Lusignan, Marc D. Benayoun, Tanya I. Baker, Adam Seth Dickey, and Nicholas G. Hatsopoulos. Matlab for Neuroscientists. New Jersey: Prentice Hall, 2009. Print.	|
|	Random Search	|	Randomly adjust parameters to ?nd a better solution than currently found.	|	Use as a benchmark for how well a search algorithm is performing. Be careful to use a good random number generator and new seed.	|	Bergstra J. and Bengio Y. Random Search for Hyper- Parameter Optimization, Journal of Machine Learning Research 13, 2012.	|
|	Regression with Shrinkage (Lasso)	|	A method of variable selection and prediction combined into a possibly biased linear model.	|	There are different methods to select the lambda parameter. A typical choice is cross validation with MSE as the metric.	|	Tibshirani, Robert. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological). Toronto: Royal Statistical Society, 1996. Print.	|
|	Sensitivity Analysis	|	Involves testing individual parameters in an analytic or model and observing the magnitude of the effect.	|	Insensitive model parameters during an optimization are candidates for being set to constants. This reduces the dimensionality of optimization problems and provides an opportunity for speed up.	|	Saltelli, A., Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. Global Sensitivity Analysis: the Primer. New Jersey: John Wiley & Sons, 2008. Print.	|
|	Simulated Annealing	|	Named after a controlled cooling process in metallurgy, and by analogy using a changing temperature or annealing schedule to vary algorithmic convergence.	|	The standard annealing function allows for initial wide exploration of the parameter space followed by a narrower search. Depending on the search priority the annealing function can be modi?ed to allow for longer explorative search at a high temperature.	|	Bertsimas, Dimitris, and John Tsitsiklis. “Simulated Annealing.” Statistical Science. 1993. Print.	|
|	Stepwise Regression	|	A method of variable selection and prediction. Akaike's information criterion AIC is used as the metric for selection. The resulting predictive model is based upon ordinary least squares, or a general linear model with parameter estimation via maximum likelihood.	|	Caution must be used when considering Stepwise Regression, as over ?tting often occurs. To mitigate over ?tting try to limit the number of free variables used.	|	Hocking, R.R. “The Analysis and Selection of Variables in Linear Regression.” Biometrics. 1976. Print.	|
|	Stochastic Gradient Descent	|	General-purpose optimization for learning of neural networks, support vector machines, and logistic regression models.	|	Applied in cases where the objective function is not completely differentiable when using sub-gradients.	|	Witten, Ian H., Eibe Frank, and Mark A. Hall. Data Mining: Practical Machine Learning Tools and Techniques. Massachusetts: Morgan Kaufmann, 2011. Print.	|
|	Support Vector Machines	|	Projection of feature vectors using a kernel function into a space where classes are more separable.	|	Try multiple kernels and use k-fold cross validation to validate the choice of the best one.	|	Hsu, Chih-Wei, Chih-Chung Chang, and Chih-Jen Lin. “A Practical Guide to Support Vector Classi?cation.” National Taiwan University Press, 2003. Print.	|
|	Term Frequency Inverse Document Frequency	|	A statistic that measures the relative importance of a term from a corpus.	|	Typically used in text mining. Assuming a corpus of news articles, a term that is very frequent such as “the” will likely appear many times in many documents, having a low value. A term that is infrequent such as a person’s last name that appears in a single article will have a higher TD IDF score.	|	Ingersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.	|
|	Topic Modeling (Latent Dirichlet Allocation)	|	Identi?es latent topics in text by examining word co-occurrence.	|	Employ part-of-speech tagging to eliminate words other than nouns and verbs. Use raw term counts instead of TF/IDF weighted terms.	|	Blei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent Dirichlet Allocation.” Journal of Machine Learning Research. 2003. Print.	|
|	Tree Based Methods	|	Models structured as graph trees where branches indicate decisions.	|	Can be used to systematize a process or act as a classi?er.	|	James, G., D. Witten, T. Hastie, and R. Tibshirani. “Tree Based Methods.” An Introduction to Statistical Learning. New York: Springer, 2013. Print.	|
|	T-Test	|	Hypothesis test used to test for differences between two groups.	|	Make sure you meet the tests assumptions and watch out for Family Wise error when running multiple tests.	|	Bhattacharyya, Gouri K., and Richard A. Johnson. Statistical Concepts and Models. Wiley, 1977. Print.	|
|	Wrapper Methods	|	Feature set reduction method that utilizes performance of a set of features on a model, as a measure of the feature set’s performance. Can help identify combinations of features in models that achieve high performance.	|	Utilize k-fold cross validation to control over ?tting.	|	John, George H., Ron Kohavi, and Karl P?eger. “Irrelevant Features and the Subset Selection Problem.” Proceedings of ICML-94, 11th International Converence on Machine Learning. New Brunswick, New Jersey. 1994. Conference Presentation.	|
